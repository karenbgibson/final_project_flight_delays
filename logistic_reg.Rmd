---
title: "Logistic regression"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r}
library(tidyverse)
library(janitor)
library(GGally)
library(modelr)
library(glmulti)
library(pROC)
library(patchwork)
```
# Data exploration and further cleaning

```{r}
newark_info <- read_csv("clean_data/newark_info.csv")

newark_info %>% 
  head()

newark_info %>% 
  distinct(flight)
```

```{r}
newark_info  %>%
  distinct(manufacturer)
# 20 manufacturers too many for factored variable. Will remove.
```


```{r}
newark_info  %>%
  distinct(type)
# 4 variables - okay to keep in.
```

```{r}
newark_info  %>%
  distinct(carrier_name)
# 10 variables - okay to keep in.
```
```{r}
newark_info  %>%
  distinct(model) %>% 
  arrange(model)
# 64 variables - too many for factored variable - further exploration needed.
# will remove for now as difficult to determine suitable groupings without domain knowledge.
```

```{r}
newark_info  %>%
  distinct(engine)
# 9 variables - okay to keep
```
```{r}
newark_info  %>%
  distinct(engines)
# 6 variables - okay to keep
```

```{r}
newark_factored <- newark_info %>% 
  select(-c(year, day, origin, dep_delay, carrier,
            dep_time, dest, sched_dep_time, tailnum,
            time_hour, date, flight, manufacturer,
            model)) %>% 
  mutate(across(c(month, weekday,
         carrier_name,
         type,
         engines,
         engine), 
                ~as.factor(.x))) %>% 
  # combining cancelled and delayed to "delayed" for binary outcome 
  mutate(delayed_or_cancelled = as.logical(
    if_else(status == "on time", FALSE, TRUE))) %>% 
  select(-status)

newark_factored %>% 
  head()
  
```
# Preparation for logistic regression

```{r}
alias(delayed_or_cancelled ~ ., data = newark_factored)
```

Will remove wind_gust as appears to be aliased variable.

Engines and engine both have more factors than showing in alias table. Engines has fewer variables and is simply a count of the number of engines in on each plane, have opted to remove this and keep the engine variable. 


```{r}
newark_factored <- newark_factored %>% 
  select(-c(wind_gust, engines))
```

```{r}
weather_factors <- newark_factored %>% 
  select(delayed_or_cancelled, wind_dir, wind_speed, visib,
         tavg_c, prcp, snow)
```

```{r}
weather_factors %>% 
  ggpairs(progress = FALSE)
```

```{r}
plane_factors <- newark_factored %>% 
  select(delayed_or_cancelled, type, engine)
```

```{r}
plane_factors %>% 
  ggpairs(progress = FALSE)
```

```{r}
schedule_factors <- newark_factored %>% 
  select(delayed_or_cancelled, month, weekday)
```

```{r}
schedule_factors %>% 
  ggpairs(progress = FALSE)
```

GG pairs plots are difficult to interpret in this instance. Manual model building not the best option. Will use glmulti package for automated model building.

```{r}
newark_factored %>% 
  head()
```

# Glmulti

Creating test and train data:

```{r}
n_rows <- nrow(newark_factored)

test_index <- sample(1:n_rows, size = n_rows*0.2)

newark_train <- slice(newark_factored, -test_index)
newark_test <- slice(newark_factored, test_index)
```

```{r}
newark_train %>%
  tabyl(delayed_or_cancelled)
```

```{r}
newark_test %>%
  tabyl(delayed_or_cancelled)
```
Test and train data have good distribution of outcome. Continue with glmulti model. 

```{r}
glmulti_search_all_mains <- glmulti(
  delayed_or_cancelled ~ ., 
  data = newark_train,
  level = 1,               # No interactions considered, main effects only
  method = "h",            # Exhaustive approach
  crit = "bic",            # BIC as criteria
  confsetsize = 10,        # Keep 10 best models
  plotty = F, 
  report = T,              # No plots, but provide interim reports
  fitfunction = "glm",     # glm function
  family = binomial(link = "logit")) # binomial family for logistic regression
```

```{r}
summary_glmulti <- summary(glmulti_search_all_mains)
```
The best model from glmulti is:

delayed_or_cancelled ~ 1 + month + weekday + carrier_name + type + hour + wind_speed + visib + tavg_c + prcp + snow"  

```{r}
mod1 <- glm(delayed_or_cancelled ~ 
              month + weekday + carrier_name + 
              type + hour +  wind_speed + visib + tavg_c + 
              prcp + snow,
            family = binomial(link = "logit"), data = newark_train)
```

```{r}
roc1 <- newark_train %>%
  add_predictions(mod1, type = "response") %>%
  roc(response = delayed_or_cancelled, predictor = pred)
```
```{r}
auc(roc1)
```
```{r}
threshold_mod1 <- 0.5

mod1_0.5 <- newark_train %>%
  add_predictions(mod1, 
                  type = "response") %>% 
  mutate(pred_thresh_0.5 = pred >= threshold)

conf_mod1 <- mod1_0.5 %>%
  tabyl(delayed_or_cancelled, pred_thresh_0.5) 

conf_mod1
```


```{r}
label_best_auc <- str_c(
  "AUC: ", round(auc(roc1),2))

(roc_curve1 <- ggroc(data = roc1, 
                     legacy.axes = TRUE, 
                     color = "darkgreen",
                     size = 2) +
   coord_fixed() + 
    labs(title = "Best model: combined factors",
         subtitle = "month, weekday, carrier, type, 
hour, wind speed visibility, average 
temperature, precipitation, snow",
x = "\n1-specificity (true negative rate)",
y = "sensitivity (true positive rate)\n"
    ) +
   annotate("text", x = 0.70, y = 0.35, 
            label = label_best_auc,
            size= 6) +
   theme_bw()) +
  theme(plot.title = element_text(
    face = "bold", size = 18),
    plot.subtitle = element_text(
      face = "bold.italic", size = 14),
    text = element_text(size = 16),
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14))
```

Performing glmulti including interaction to test if model is better:

```{r}
glmulti_search_all_mains_one_pair <- glmulti(
  delayed_or_cancelled ~ month + weekday + carrier_name + 
              type + hour +  wind_speed + visib + tavg_c + 
              prcp + snow,
  data = newark_train,
  level = 2,               # Interactions considered
  method = "h",            # Exhaustive approach
  crit = "bic",            # BIC as criteria
  confsetsize = 10,        # Keep 10 best models
  marginality = TRUE,      # consider pairs only if both main effects in model
  minsize = 11,             # minsize, maxsize and marginality here force 
  maxsize = 11,             # inclusion of a single pair beyond the ten main effects
  plotty = F, 
  report = T,              # No plots, but provide interim reports
  fitfunction = "glm",     # glm function
  family = binomial(link = "logit")) # binomial family for logistic regression

```
Interrupted after 12 hours of running:

After 63800 models:
Best model: delayed_or_cancelled~1+month+hour+wind_speed+visib+tavg_c+prcp+snow+tavg_c:hour+snow:prcp+month:tavg_c+month:prcp
Crit= 87583.5569166449
Mean crit= 87625.7595997976

```{r}
mod2 <- glm(delayed_or_cancelled ~ month + hour + wind_speed + visib + tavg_c + prcp + snow + tavg_c:hour + snow:prcp + month:tavg_c + month:prcp,
            family = binomial(link = "logit"), data = newark_train)
```

```{r}
roc2 <- newark_train %>%
  add_predictions(mod2, type = "response") %>%
  roc(response = delayed_or_cancelled, predictor = pred)
```

```{r}
auc(roc2)
```
AUC is smaller than model without interactions. Interaction model not competitive. 

## Testing mod1 on training data

```{r}
roc1_test <- newark_test %>%
  add_predictions(mod1, type = "response") %>%
  roc(response = delayed_or_cancelled, predictor = pred)
```
```{r}
auc(roc1_test)
```
Area under curve for training data: 0.7526
Area under curve for test data: 0.7542

Model seems to fit data well. 

## Logistic regression with weather variables only - backward selection

With all weather related variables:

```{r}
mod1_weather <- glm(delayed_or_cancelled ~ 
                    wind_dir + wind_speed + 
                    visib + tavg_c + prcp + snow,
                    data = newark_train, 
                    family = binomial(link = "logit"))

summary(mod1_weather)
```

```{r}
bic(mod1_weather)
```

wind_dir is not significant - remove. BIC is much higher than that of glmulti model with additional (non-weather) factors. 

```{r}
mod2_weather <- glm(delayed_or_cancelled ~ wind_speed + 
                    visib + tavg_c + prcp + snow,
                    data = newark_train, 
                    family = binomial(link = "logit"))

summary(mod2_weather)
```

```{r}
bic(mod2_weather)
```
BIC still much higher that glmulti model. 

Further testing of weather only model (mod2_weather):

```{r}
roc_weather <- newark_train %>%
  add_predictions(mod2_weather, type = "response") %>%
  roc(response = delayed_or_cancelled, predictor = pred)
```
```{r}
auc(roc_weather)
```

```{r}
roc_weather_test <- newark_test %>%
  add_predictions(mod2_weather, type = "response") %>%
  roc(response = delayed_or_cancelled, predictor = pred)
```
```{r}
auc(roc_weather_test)
```
AUC train: 0.6018
AUC test: 0.5938

Model fits data well.

```{r}
#plotting roc curve for best weather model

label_weather_auc <- str_c(
  "AUC: ", round(auc(roc_weather_test),2))

(roc_curve_weather <- ggroc(data = roc_weather, 
                            legacy.axes = TRUE, 
                            colour = "#0072B2",
                            size = 2) +
    coord_fixed() + 
    labs(title = "Model: weather related factors",
         subtitle = "wind speed, visibility, average 
temperature, precipitation & snow\n",
x = "\n1-specificity (true negative rate)",
y = "sensitivity (true positive rate)\n"
    ) +
    annotate("text", x = 0.70, y = 0.35, 
             label = label_weather_auc,
             size= 6) +
    theme_bw()) +
  theme(plot.title = element_text(
    face = "bold", size = 18),
    plot.subtitle = element_text(
      face = "bold.italic", size = 14),
    text = element_text(size = 16),
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14))
```

```{r}
roc_curve_weather + roc_curve1

ggsave("roc_curves.png", width = 10, height = 6)
```

## Weather factors and cancelled flights

```{r}
newark_factored_cancelled <- newark_info %>% 
  select(-c(year, day, origin, dep_delay, carrier,
            dep_time, dest, sched_dep_time, tailnum,
            time_hour, date, flight, manufacturer,
            model)) %>% 
  mutate(across(c(month, weekday,
         carrier_name,
         type,
         engines,
         engine), 
                ~as.factor(.x))) %>% 
  # adding variable for cancelled, if false then delayed or on time
  mutate(cancelled = as.logical(
    if_else(status == "cancelled", TRUE, FALSE))) %>% 
  select(-status)
```

Creating new test and training data:

```{r}
n_rows_cancelled <- nrow(newark_factored_cancelled)

test_index_cancelled <- sample(1:n_rows, size = n_rows*0.2)

newark_train_cancelled <- slice(
  newark_factored_cancelled, -test_index)
newark_test_cancelled <- slice(
  newark_factored_cancelled, test_index)
```

```{r}
newark_train_cancelled %>%
  tabyl(cancelled)
```

```{r}
newark_test_cancelled %>%
  tabyl(cancelled)
```
Equal distribution in test and train data. 

```{r}
mod1_weather_cancelled <- glm(cancelled ~ 
                    wind_dir + wind_speed + 
                    visib + tavg_c + prcp + snow,
                    data = newark_train_cancelled, 
                    family = binomial(link = "logit"))

summary(mod1_weather_cancelled)
```
All weather factors have significant p-values. 

```{r}
bic(mod1_weather_cancelled)
```
```{r}
roc_weather_cancelled <- newark_train_cancelled %>%
  add_predictions(mod1_weather_cancelled, 
                  type = "response") %>%
  roc(response = cancelled, predictor = pred)
```
```{r}
auc(roc_weather_cancelled)
```
Confusion matrix for cancelled model

```{r}
threshold <- 0.5

weather_cancelled_0.5 <- newark_train_cancelled %>%
  add_predictions(mod1_weather_cancelled, 
                  type = "response") %>% 
  mutate(pred_thresh_0.5 = pred >= threshold)

conf_weather_cancelled <- weather_cancelled_0.5 %>%
  tabyl(cancelled, pred_thresh_0.5) 

conf_weather_cancelled
```
# Removing on time and setting binary classifier to cancelled or delayed

```{r}
newark_cancelled <- newark_info %>% 
  filter(status != "on time") %>% 
  select(-c(year, day, origin, dep_delay, carrier,
            dep_time, dest, sched_dep_time, tailnum,
            time_hour, date, flight, manufacturer,
            model)) %>% 
  mutate(across(c(month, weekday,
         carrier_name,
         type,
         engines,
         engine), 
                ~as.factor(.x))) %>% 
  # adding variable for cancelled, if false then delayed only
  mutate(cancelled = as.logical(
    if_else(status == "cancelled", TRUE, FALSE))) %>% 
  select(-status)
```

Creating new test and training data:

```{r}
n_rows_cancelled2 <- nrow(newark_cancelled)

test_index_cancelled2 <- sample(1:n_rows, size = n_rows*0.2)

train_cancelled2 <- slice(
  newark_cancelled, -test_index)

test_cancelled2 <- slice(
  newark_cancelled, test_index)
```

```{r}
train_cancelled2 %>%
  tabyl(cancelled)
```

```{r}
test_cancelled2 %>%
  tabyl(cancelled)
```
Equal distribution in test and train data. 

```{r}
mod1_cancelled <- glm(cancelled ~ 
                    wind_dir + wind_speed + 
                    visib + tavg_c + prcp + snow,
                    data = newark_cancelled, 
                    family = binomial(link = "logit"))

summary(mod1_cancelled)
```
All weather factors have significant p-values. 

```{r}
bic(mod1_cancelled)
```

```{r}
roc_cancelled <- newark_cancelled %>%
  add_predictions(mod1_cancelled, 
                  type = "response") %>%
  roc(response = cancelled, predictor = pred)
```

```{r}
auc(roc_cancelled)
```

Confusion matrix for cancelled only model

```{r}
threshold2 <- 0.5

cancelled_0.5 <- newark_cancelled %>%
  add_predictions(mod1_cancelled, 
                  type = "response") %>% 
  mutate(pred_thresh_0.5 = pred >= threshold)

conf_cancelled <- cancelled_0.5 %>%
  tabyl(cancelled, pred_thresh_0.5) 

conf_cancelled
```
Although AUC is 0.7, model is predicting more false positives than true negatives therefore model is not suitable. 


